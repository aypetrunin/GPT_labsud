{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title Инсталяция библиотек.\n",
        "#!pip  install  tiktoken==0.4.0  langchain==0.0.231 openai==0.27.8 chromadb gspread oauth2client nltk pydantic==1.10.8 faiss-cpu==1.7.4\n",
        "!pip  install  tiktoken\n",
        "!pip  install  langchain\n",
        "!pip  install  openai\n",
        "!pip  install  chromadb\n",
        "!pip  install  gspread\n",
        "!pip  install  oauth2client\n",
        "!pip  install  nltk\n",
        "!pip  install  pydantic\n",
        "!pip  install  faiss-cpu"
      ],
      "metadata": {
        "id": "u48WibPWUdVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71d77ca8-0d55-434b-801b-5e8aca7e5d9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Installing collected packages: tiktoken\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tiktoken-0.5.1\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.0.338-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.23)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.2-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.63 (from langchain)\n",
            "  Downloading langsmith-0.0.65-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain) (1.1.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.5.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.7,>=0.5.7->langchain) (23.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.6.2 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.338 langsmith-0.0.65 marshmallow-3.20.1 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting openai\n",
            "  Downloading openai-1.3.3-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.3\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.4.17-py3-none-any.whl (496 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m496.8/496.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.28 in /usr/local/lib/python3.10/dist-packages (from chromadb) (2.31.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.10.13)\n",
            "Collecting chroma-hnswlib==0.7.3 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.104.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn[standard]>=0.18.3 (from chromadb)\n",
            "  Downloading uvicorn-0.24.0.post1-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.0.2-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.5.0)\n",
            "Collecting pulsar-client>=3.1.0 (from chromadb)\n",
            "  Downloading pulsar_client-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.21.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.21.0-py3-none-any.whl (18 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.21.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.3/105.3 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.15.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (4.66.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.4.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.1.1)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.59.2)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m593.7/593.7 kB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (0.9.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-28.1.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.10/dist-packages (from chromadb) (8.2.3)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from chromadb) (6.0.1)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.10/dist-packages (from chromadb) (1.23.5)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi>=0.95.2->chromadb) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.5.0 (from chromadb)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2023.7.22)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (2.17.3)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.6.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Collecting urllib3<2.0,>=1.24.2 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: importlib-metadata<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (6.8.0)\n",
            "Collecting backoff<3.0.0,>=1.10.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.10/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.61.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.21.0-py3-none-any.whl (17 kB)\n",
            "Collecting opentelemetry-proto==1.21.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.21.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opentelemetry-semantic-conventions==0.42b0 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.42b0-py3-none-any.whl (36 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.28->chromadb) (3.4)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.2->chromadb) (0.19.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9.0->chromadb) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\n",
            "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m50.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0.0,>=3.7.1->fastapi>=0.95.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.2.0->chromadb) (1.14.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2023.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.0)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=9bd4c5dab21d77d8205bf10494826b722e3c43f8c72d004f5b1bd718e0a166d5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, websockets, uvloop, urllib3, typing-extensions, python-dotenv, pulsar-client, overrides, opentelemetry-semantic-conventions, opentelemetry-proto, humanfriendly, httptools, deprecated, chroma-hnswlib, bcrypt, backoff, watchfiles, uvicorn, starlette, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, coloredlogs, posthog, opentelemetry-sdk, onnxruntime, fastapi, opentelemetry-exporter-otlp-proto-grpc, kubernetes, chromadb\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.5.0\n",
            "    Uninstalling typing_extensions-4.5.0:\n",
            "      Successfully uninstalled typing_extensions-4.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed backoff-2.2.1 bcrypt-4.0.1 chroma-hnswlib-0.7.3 chromadb-0.4.17 coloredlogs-15.0.1 deprecated-1.2.14 fastapi-0.104.1 httptools-0.6.1 humanfriendly-10.0 kubernetes-28.1.0 monotonic-1.6 onnxruntime-1.16.2 opentelemetry-api-1.21.0 opentelemetry-exporter-otlp-proto-common-1.21.0 opentelemetry-exporter-otlp-proto-grpc-1.21.0 opentelemetry-proto-1.21.0 opentelemetry-sdk-1.21.0 opentelemetry-semantic-conventions-0.42b0 overrides-7.4.0 posthog-3.0.2 pulsar-client-3.3.0 pypika-0.48.9 python-dotenv-1.0.0 starlette-0.27.0 typing-extensions-4.8.0 urllib3-1.26.18 uvicorn-0.24.0.post1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.10/dist-packages (3.4.2)\n",
            "Requirement already satisfied: requests>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from gspread) (2.31.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from gspread) (2.17.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.2.1->gspread) (2023.7.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->gspread) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->gspread) (0.5.0)\n",
            "Requirement already satisfied: oauth2client in /usr/local/lib/python3.10/dist-packages (4.1.3)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.22.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (0.3.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (4.9)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from oauth2client) (1.16.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.10/dist-packages (from httplib2>=0.9.1->oauth2client) (3.1.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.13)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.8.0)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.7.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list"
      ],
      "metadata": {
        "id": "cZsjvCXkJXq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Импорт библиотек.\n",
        "\n",
        "#from langchain.llms import OpenAI\n",
        "#from langchain.document_loaders import UnstructuredMarkdownLoader\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "import gdown\n",
        "import requests\n",
        "import pathlib\n",
        "import subprocess\n",
        "import tempfile\n",
        "import ipywidgets as widgets\n",
        "import os\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import re\n",
        "import getpass\n",
        "import os\n",
        "import openai\n",
        "import tiktoken\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "iO9CMx_FvOOq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Подключение к OpenAI API.\n",
        "# # Получение ключа API от пользователя и установка его как переменной окружения\n",
        "# openai_key = getpass.getpass(\"OpenAI API Key:\")\n",
        "# os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
        "# #openai.api_key = openai_key\n",
        "\n",
        "# Установка OpenAI API key\n",
        "openai.api_key = getpass.getpass(\"Введите OpenAi API key:\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai.api_key\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9jZlKHIWRsk",
        "outputId": "5b07cbfd-20ea-40e0-cbb5-29e00de720d6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Введите OpenAi API key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Вспомогательные функции\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Возвращает количество токенов в строке\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "def split_text(text, max_count, count_type, verbose=0):\n",
        "    \"\"\" Функция разбивает текст на чанки. \"\"\"\n",
        "\n",
        "    # Функция для подсчета количества слов в фрагменте.\n",
        "    def num_words(fragment):\n",
        "        return  len(fragment.split())\n",
        "\n",
        "    # Функция для подсчета количества токенов в фрагменте.\n",
        "    def num_tokens(fragment):\n",
        "        return num_tokens_from_string(fragment, \"cl100k_base\")\n",
        "\n",
        "    # Разбивка на чанки происходит в два этапа.\n",
        "    # На Первом этапе в формате Markdown делится на чанки по по делителям заголовков и подзаголовков.\n",
        "    # На Втором  этапе полученные чанки делятся при необходимости на более мелкие.\n",
        "\n",
        "    # Первый этап.\n",
        "    # Шаблон для MarkdownHeaderTextSplitter по которому будет делится переданный текст в формате Markdown.\n",
        "    headers_to_split_on = [(\"#\", \"Header 1\"), (\"##\", \"Header 2\"), (\"###\", \"Header 3\")]\n",
        "    # Создаем экземпляр спилиттера.\n",
        "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "    # Получаем список чанков.\n",
        "    fragments = markdown_splitter.split_text(text)\n",
        "\n",
        "    # Второй этап.\n",
        "    # Выбор функции подсчета длины в зависимости от типа подсчета.\n",
        "    length_function = num_words if count_type == \"words\" else num_tokens\n",
        "    # Создание объекта разделителя текста.\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=max_count, chunk_overlap=0, length_function=length_function)\n",
        "    # Список для хранения фрагментов текста.\n",
        "    source_chunks = splitter.split_documents(fragments)\n",
        "\n",
        "    # Обработка каждого фрагмента текста.\n",
        "    if verbose:\n",
        "      for chank in source_chunks:\n",
        "            # Вывод количества слов/токенов в фрагменте, если включен режим verbose.\n",
        "            count = length_function(chank.page_content)\n",
        "            answer = f\"{count_type} in text fragment = {count}\\n{'-' * 5}\\n{chank}\\n{'=' * 20}\"\n",
        "            print(insert_newlines(answer))\n",
        "\n",
        "    # Возвращение списка фрагментов текста.\n",
        "    return source_chunks\n",
        "\n",
        "\n",
        "def create_embedding(data, max_count, count_type, verbose=0):\n",
        "    source_chunks = []\n",
        "    source_chunks = split_text(text=data, max_count=max_count, count_type=count_type, verbose=verbose)\n",
        "\n",
        "    # Создание индексов документа\n",
        "    search_index = FAISS.from_documents(source_chunks, OpenAIEmbeddings(), )\n",
        "\n",
        "    count_token = num_tokens_from_string(' '.join([x.page_content for x in source_chunks]), \"cl100k_base\")\n",
        "    print('\\n ===========================================: ')\n",
        "    print('Количество токенов в документе :', count_token)\n",
        "    print('ЦЕНА запроса:', 0.0004*(count_token/1000), ' $')\n",
        "    return search_index\n",
        "\n",
        "def load_search_indexes(url: str, max_count, count_type, verbose=0) -> str:\n",
        "    \"\"\"Download the document as plain text.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url) # Получение документа по url.\n",
        "        response.raise_for_status()  # Проверка ответа и если была ошибка - формирование исключения.\n",
        "        # Создание векторной База знаний.\n",
        "        search_index = create_embedding(\n",
        "                          response.text,\n",
        "                          max_count=max_count,\n",
        "                          count_type=count_type,\n",
        "                          verbose=verbose)\n",
        "        return search_index\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def load_file(url: str) -> str:\n",
        "    try:\n",
        "        response = requests.get(url) # Получение документа по url.\n",
        "        response.raise_for_status()  # Проверка ответа и если была ошибка - формирование исключения.\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
        "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
        "    try:\n",
        "        encoding = tiktoken.encoding_for_model(model)\n",
        "    except KeyError:\n",
        "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n",
        "        num_tokens = 0\n",
        "        for message in messages:\n",
        "            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
        "            for key, value in message.items():\n",
        "                num_tokens += len(encoding.encode(value))\n",
        "                if key == \"name\":  # if there's a name, the role is omitted\n",
        "                    num_tokens += -1  # role is always required and always 1 token\n",
        "        num_tokens += 2  # every reply is primed with <im_start>assistant\n",
        "        return num_tokens\n",
        "    else:\n",
        "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\"\"\")\n",
        "\n",
        "def insert_newlines(text: str, max_len: int = 120) -> str:\n",
        "    \"\"\" Функция форматирует переданный текст по длине\n",
        "    для лучшего восприятия на экране.\"\"\"\n",
        "    words = text.split()\n",
        "    lines = []\n",
        "    current_line = \"\"\n",
        "    for word in words:\n",
        "        if len(current_line + \" \" + word) > max_len:\n",
        "            lines.append(current_line)\n",
        "            current_line = \"\"\n",
        "        current_line += \" \" + word\n",
        "    lines.append(current_line)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def answer_index(system, topic, search_index, temp = 1, verbose = 0, top_similar_documents = 5):\n",
        "    \"\"\" Основная функция которая формирует запрос и получает ответ от OpenAI по заданному вопросу\n",
        "    на основе векторной Базы знаний. \"\"\"\n",
        "\n",
        "    # Выборка чанков по схожести с вопросом.\n",
        "    docs = search_index.similarity_search(topic, k=top_similar_documents)\n",
        "\n",
        "    # Формирование контекста на основе выбранных чанков.\n",
        "    message_content = re.sub(r'\\n{2}', ' ', '\\n '.join([f'\\nОтрывок документа №{i+1}\\n=====================' + doc.page_content + '\\n' for i, doc in enumerate(docs)]))\n",
        "\n",
        "    if (verbose):\n",
        "        print('\\n ===========================================: ')\n",
        "        print('message_content :\\n ======================================== \\n', message_content)\n",
        "    # Формирование запроса к OpenAI.\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system + f\"{message_content}\"},\n",
        "        {\"role\": \"user\", \"content\": topic}\n",
        "    ]\n",
        "    # Отправка запроса к Open AI.\n",
        "    client = OpenAI(\n",
        "        api_key=openai.api_key\n",
        "    )\n",
        "    completion = client.chat.completions.create(\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      messages=messages,\n",
        "      temperature=temp\n",
        "    )\n",
        "    # Вывод на экран подробностей при необходимости.\n",
        "    if (verbose):\n",
        "        print('\\n ===========================================: ')\n",
        "        print(f'{completion.usage.prompt_tokens} токенов использовано на вопрос.')\n",
        "        print('\\n ===========================================: ')\n",
        "        print(f'{completion.usage.total_tokens} токенов использовано всего (вопрос-ответ).')\n",
        "        print('\\n ===========================================: ')\n",
        "        print('ЦЕНА запроса с ответом :', 0.002*(completion.usage.total_tokens/1000), ' $')\n",
        "        print('\\n ===========================================: ')\n",
        "    # Ответ OpenAI.\n",
        "    answer = 'ОТВЕТ : \\n' + insert_newlines(completion.choices[0].message.content)\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "wr9jQm4dvOeR"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Данные для формирования векторной Базы знаний.\n",
        "count_type = \"tokens\"       # \"words\", \"token\". Тип разбивки на чанки.\n",
        "max_count = 2000            # Максимальное число слов или токенов.\n",
        "verbose_bd = 1              # Показывать полученные чанки.\n",
        "federallab_bd = \"https://raw.githubusercontent.com/terrainternship/GPT_labsud/main/Datadase/LabSudDB_v1.md\" # Сылка на Базу знаний на Github.\n",
        "\n",
        "# Разбивка Базы знаний идет в два этапа:\n",
        "# 1. На первом этапе разбивка идет сплитером - MarkdownHeaderTextSplitter. - https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/markdown_header_metadata\n",
        "#    Текст разбивается на чанки по заголовкам (#, ##, ##) с указанием в метаданных всей иерархии заголовков для этого чанка. Размер этого чанка равен всему объему текста между заголовками.\n",
        "# 2. На втором этапе чанки после первого этапа разбиваются на чанки сплитером - RecursiveCharacterTextSplitter. - https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter\n",
        "#    Если нет необходимости во втором этапе и вся работа была проведена при формировании Базы знаний по ограничению чанков, то нужно поставить сначение в параметре 'max_count' заведомо большим."
      ],
      "metadata": {
        "id": "jFOoW7AKvOjH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Формирование векторной Базы знаний.\n",
        "\n",
        "federallab_bd_index = load_search_indexes(federallab_bd, max_count=max_count, count_type=count_type, verbose=verbose_bd)"
      ],
      "metadata": {
        "id": "xTFqGmeXvpbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Формулировка промта для нейро-консультанта.\n",
        "\n",
        "federallab_chat_promt1 = '''Ты консультант компании судебной экспертизы.\n",
        "У тебя есть подробная информация по услугам и ценам.\n",
        "Тебе задает вопрос клиент, дай ему информацию, опираясь на предоставленные материалы.\n",
        "Отвечай максимально точно и используй только информацию из документов, не добавляй ничего своего.\n",
        "Неупоминай этот документ в ответе.\n",
        "Документ с информацией для ответа соискателю: '''\n",
        "\n",
        "federallab_chat_promt2 = load_file(\"https://raw.githubusercontent.com/terrainternship/GPT_labsud/main/Dokumov/%D0%A2%D1%8B%20%D1%81%D0%B0%D0%BC%D1%8B%D0%B9%20%D0%BA%D0%BE%D0%BC%D0%BF%D0%B5%D1%82%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9%20%D0%BD%D0%B5%D0%B9%D1%80%D0%BE-%D0%BA%D0%BE%D0%BD%D1%81%D1%83%D0%BB%D1%8C.txt\")\n",
        "federallab_chat_promt3 = load_file(\"https://raw.githubusercontent.com/terrainternship/GPT_labsud/main/Galina/FLSE_promt\")\n"
      ],
      "metadata": {
        "id": "lS_hValxvpeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Вопрос нейро-консультанту:\n",
        "\n",
        "question = 'Прорводите транспортно-трасологическую экспертизу'"
      ],
      "metadata": {
        "id": "-u9vFYURknyj"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Отправка запроса нейро-консультанту.\n",
        "\n",
        "top_similar_documents = 3   # Количество полученных релевантных чанков после запроса.\n",
        "verbose = 1 # Показывать отобранные чанки.\n",
        "temp = 0    # Вариативность ответа.\n",
        "\n"
      ],
      "metadata": {
        "id": "JHJLOxt0lpbj"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer2 = answer_index(\n",
        "    system = federallab_chat_promt2,\n",
        "    topic = question,\n",
        "    search_index = federallab_bd_index,\n",
        "    temp = temp,\n",
        "    verbose = verbose,\n",
        "    top_similar_documents = top_similar_documents\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anmfl4qteI7Q",
        "outputId": "a0ac4031-8016-4479-d0ee-2e47084abd8a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ===========================================: \n",
            "message_content :\n",
            " ======================================== \n",
            " \n",
            "Отрывок документа №1\n",
            "=====================Экспертиза обстоятельств ДТП и следов ТС.\n",
            "Независимая автотехническая экспертиза  \n",
            "Предметом исследования при проведении экспертизы обстоятельств ДТП являются дорожно-транспортные ситуации (ДТС), параметры движения ТС, иных объектов и пешеходов в процессе ДТП, а также действия и возможности водителей. При транспортно-трасологической экспертизе исследованию подлежат следы различных видов транспорта (от колес, гусениц, выступающих частей ТС, частиц ТС) и механизмы их образования.  \n",
            "Объектами экспертизы могут быть: дорожное покрытие, транспортные средства, одежда и тело человека, дорожные ограждения и знаки, придорожные постройки.  \n",
            "Примерные вопросы на разрешение эксперта-трасолога:  \n",
            "Определить механизм столкновения транспортных средств, представленных на транспортно-трасологическую экспертизу (удар, скольжение, волочение и пр.)?\n",
            "Определить, имело ли место столкновение представленных на исследование ТС, исходя из характера и локализации повреждений на данных ТС?\n",
            "Определить последовательность возникновения повреждений (следов) при столкновении транспортного средства о преграду?\n",
            "Определить угол взаимного расположения ТС в момент их первоначального контакта?\n",
            "Определить, в каком направлении двигалось транспортное средство в момент наезда?\n",
            "Определить траекторию и характер движения транспортных средств?\n",
            "Определить, где, относительно границ проезжей части, находится место столкновения (место наезда)?\n",
            "Определить расположение транспортных средств и препятствий в момент удара (или в другие заданные моменты времени)?\n",
            "Определить, взаимное расположение транспортных средств в момент столкновения?\n",
            "Определить, находилось в движении или было неподвижно ТС в момент столкновения с ним другого ТС?\n",
            "Определить, какой деталью (частью) транспортного средства оставлены следы?\n",
            "Определить, каким видам транспорта, судя по следам, присущи такие детали (такого размера, формы, локализации)?\n",
            "Определить, транспортом какого типа (легковым, грузовым, гусеничным и т.д.) оставлены следы?\n",
            "Определить, какой частью транспортного средства нанесены повреждения?\n",
            "Исследование следов шин транспортных средств\n",
            "Определить, модель шины, которой был оставлен след, слепок которого представлен на транспортно-трасологическую экспертизу?\n",
            "Определить, к какому типу (модели или марки) относится транспортное средство, оставившее следы на месте ДТП ?\n",
            "Определить, в каком направлении от места ДТП двигалось данное ТС, оставившее следы колес на том или ином участке дороги?\n",
            "Определить, не могли ли данные следы быть оставлены такими-то колесами (передними, задними, правыми, левыми) определенного транспортного средства?\n",
            "Определить, являются ли данные следы результатом воздействия шин транспортного средства?\n",
            "Определить, какие дефекты имеют шины, оставившие следы на месте ДТП?\n",
            "Определить, отличительные признаки шин, отобразившиеся в следах (например дефекты, степень износа протектора, неравномерный износ протектора вследствие неправильной регулировки углов установки или дисбаланса колес, пробои и порезы, вздутие протектора и др.)?\n",
            "Определить, грузовым или легковым автомобилем были оставлены следы?  \n",
            "Отрывок документа №2\n",
            "=====================Экспертиза обстоятельств ДТП. Услуга экспертиза обстоятельств ДТП. Что включает услуга.  \n",
            "Процедура представляет собой целый комплекс сложных мероприятий. Чтобы полностью воссоздать картину и доказать невинность или виновность специалистам необходимо:  \n",
            "Определить всех участников ДТП, скорость движения авто, установить остановочный путь.\n",
            "Воссоздать картину решений принятых участниками ДТП – определить соответствовали ли данные действия правилам дорожного движения, также будет дана оценка скорости принятия решений.\n",
            "Проанализировать возможность потери управляемости авто из-за технических повреждений, для этого проводятся диагностические работы с транспортным средством.\n",
            "Установить реконструкцию происшествия использовав трасологический анализ.\n",
            "Оценить показания свидетелей на факт правдивости. Если расхождения будут обнаружены установить и зарегистрировать данный момент.\n",
            "Проанализировать погодные условия в день аварии.\n",
            "Оценить качество дорожного полотна на участке.\n",
            "Проанализировать ДТП с учётом дополнительных факторов – наличие пешеходов.  \n",
            "Итогом работы является развернутое заключение с исходными данными, подробным описанием исследований, которые были реализованы, расчетной составляющей, и выводами. Если это необходимо к документам прилагают фотоматериалы, чертежи и схемы.  \n",
            "Отрывок документа №3\n",
            "=====================Экспертиза технического состояния ТС.\n",
            "В рамках экспертизы технического состояния ТС может быть проведена:\n",
            "Экспертиза технического состояния транспортных средств выполняется по следующим основным направлениям:  \n",
            "в связи с совершенным дорожно-транспортным происшествием,\n",
            "при возникновении сомнений в качестве транспортного средства или выявлении дефектов в процессе его эксплуатации.  \n",
            "Проведение экспертизы технического состояния транспортных средств позволяет:  \n",
            "определить наличие или отсутствие неисправности, поломки, дефекта в обследуемом транспортном средстве (автомобиле);\n",
            "определить, является ли выявленная неисправность, поломка или дефект в обследуемом транспортном средстве (авто) приобретенной в ходе неправильной эксплуатации автомашины, либо же данная неисправность (дефект, недостаток) имеет заводской характер;\n",
            "определить время (момент) возникновения неисправности, поломки или дефекта относительно ДТП (до или после);\n",
            "определить причину возникновения неисправности, поломки или дефекта относительно характера возникновения (заводской дефект транспортного средства или дефект в процессе эксплуатации автомобиля);\n",
            "определить причину возникновения неисправности, поломки или дефекта относительно причины возникновения (с чем именно произошло столкновение);\n",
            "установить в период какого технологического заводского цикла дефекты могли возникнуть (в момент проектирования или конструирования автомобиля, либо в момент литья узлов и деталей, либо в момент некачественной или неправильной сборки узлов и агрегатов автомашины) если неисправность, поломка или дефект, имеет ярко выраженное заводское происхождение\n",
            "установить в чем конкретно выражена неправильная эксплуатация транспортного средства, если неисправности, поломки или дефекты, являются приобретенными в ходе неправильной эксплуатации транспортного средства\n",
            "установить причинно-следственные связи между неисправностью и ДТП, а также обстоятельствами, способствующими возникновению неисправностей\n",
            "установить техническую возможность предотвращения ДТП (наезда, столкновения, потери устойчивости и т.п.) при определенном техническом состоянии ТС, их отдельных узлов, механизмов, систем, агрегатов, в момент ДТП\n",
            "установить обстоятельства, связанные с техническим состоянием ТС, которые способствовали или могли способствовать возникновению ДТП.\n",
            "\n",
            "\n",
            " ===========================================: \n",
            "3561 токенов использовано на вопрос.\n",
            "\n",
            " ===========================================: \n",
            "3652 токенов использовано всего (вопрос-ответ).\n",
            "\n",
            " ===========================================: \n",
            "ЦЕНА запроса с ответом : 0.007304000000000001  $\n",
            "\n",
            " ===========================================: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "answer3 = answer_index(\n",
        "    system = federallab_chat_promt3,\n",
        "    topic = question,\n",
        "    search_index = federallab_bd_index,\n",
        "    temp = temp,\n",
        "    verbose = verbose,\n",
        "    top_similar_documents = top_similar_documents\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHddVCkPeMX1",
        "outputId": "ea6d28a6-dfdf-4f33-eef8-c9f8239c4b98"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ===========================================: \n",
            "message_content :\n",
            " ======================================== \n",
            " \n",
            "Отрывок документа №1\n",
            "=====================Экспертиза обстоятельств ДТП и следов ТС.\n",
            "Независимая автотехническая экспертиза  \n",
            "Предметом исследования при проведении экспертизы обстоятельств ДТП являются дорожно-транспортные ситуации (ДТС), параметры движения ТС, иных объектов и пешеходов в процессе ДТП, а также действия и возможности водителей. При транспортно-трасологической экспертизе исследованию подлежат следы различных видов транспорта (от колес, гусениц, выступающих частей ТС, частиц ТС) и механизмы их образования.  \n",
            "Объектами экспертизы могут быть: дорожное покрытие, транспортные средства, одежда и тело человека, дорожные ограждения и знаки, придорожные постройки.  \n",
            "Примерные вопросы на разрешение эксперта-трасолога:  \n",
            "Определить механизм столкновения транспортных средств, представленных на транспортно-трасологическую экспертизу (удар, скольжение, волочение и пр.)?\n",
            "Определить, имело ли место столкновение представленных на исследование ТС, исходя из характера и локализации повреждений на данных ТС?\n",
            "Определить последовательность возникновения повреждений (следов) при столкновении транспортного средства о преграду?\n",
            "Определить угол взаимного расположения ТС в момент их первоначального контакта?\n",
            "Определить, в каком направлении двигалось транспортное средство в момент наезда?\n",
            "Определить траекторию и характер движения транспортных средств?\n",
            "Определить, где, относительно границ проезжей части, находится место столкновения (место наезда)?\n",
            "Определить расположение транспортных средств и препятствий в момент удара (или в другие заданные моменты времени)?\n",
            "Определить, взаимное расположение транспортных средств в момент столкновения?\n",
            "Определить, находилось в движении или было неподвижно ТС в момент столкновения с ним другого ТС?\n",
            "Определить, какой деталью (частью) транспортного средства оставлены следы?\n",
            "Определить, каким видам транспорта, судя по следам, присущи такие детали (такого размера, формы, локализации)?\n",
            "Определить, транспортом какого типа (легковым, грузовым, гусеничным и т.д.) оставлены следы?\n",
            "Определить, какой частью транспортного средства нанесены повреждения?\n",
            "Исследование следов шин транспортных средств\n",
            "Определить, модель шины, которой был оставлен след, слепок которого представлен на транспортно-трасологическую экспертизу?\n",
            "Определить, к какому типу (модели или марки) относится транспортное средство, оставившее следы на месте ДТП ?\n",
            "Определить, в каком направлении от места ДТП двигалось данное ТС, оставившее следы колес на том или ином участке дороги?\n",
            "Определить, не могли ли данные следы быть оставлены такими-то колесами (передними, задними, правыми, левыми) определенного транспортного средства?\n",
            "Определить, являются ли данные следы результатом воздействия шин транспортного средства?\n",
            "Определить, какие дефекты имеют шины, оставившие следы на месте ДТП?\n",
            "Определить, отличительные признаки шин, отобразившиеся в следах (например дефекты, степень износа протектора, неравномерный износ протектора вследствие неправильной регулировки углов установки или дисбаланса колес, пробои и порезы, вздутие протектора и др.)?\n",
            "Определить, грузовым или легковым автомобилем были оставлены следы?  \n",
            "Отрывок документа №2\n",
            "=====================Экспертиза обстоятельств ДТП. Услуга экспертиза обстоятельств ДТП. Что включает услуга.  \n",
            "Процедура представляет собой целый комплекс сложных мероприятий. Чтобы полностью воссоздать картину и доказать невинность или виновность специалистам необходимо:  \n",
            "Определить всех участников ДТП, скорость движения авто, установить остановочный путь.\n",
            "Воссоздать картину решений принятых участниками ДТП – определить соответствовали ли данные действия правилам дорожного движения, также будет дана оценка скорости принятия решений.\n",
            "Проанализировать возможность потери управляемости авто из-за технических повреждений, для этого проводятся диагностические работы с транспортным средством.\n",
            "Установить реконструкцию происшествия использовав трасологический анализ.\n",
            "Оценить показания свидетелей на факт правдивости. Если расхождения будут обнаружены установить и зарегистрировать данный момент.\n",
            "Проанализировать погодные условия в день аварии.\n",
            "Оценить качество дорожного полотна на участке.\n",
            "Проанализировать ДТП с учётом дополнительных факторов – наличие пешеходов.  \n",
            "Итогом работы является развернутое заключение с исходными данными, подробным описанием исследований, которые были реализованы, расчетной составляющей, и выводами. Если это необходимо к документам прилагают фотоматериалы, чертежи и схемы.  \n",
            "Отрывок документа №3\n",
            "=====================Экспертиза технического состояния ТС.\n",
            "В рамках экспертизы технического состояния ТС может быть проведена:\n",
            "Экспертиза технического состояния транспортных средств выполняется по следующим основным направлениям:  \n",
            "в связи с совершенным дорожно-транспортным происшествием,\n",
            "при возникновении сомнений в качестве транспортного средства или выявлении дефектов в процессе его эксплуатации.  \n",
            "Проведение экспертизы технического состояния транспортных средств позволяет:  \n",
            "определить наличие или отсутствие неисправности, поломки, дефекта в обследуемом транспортном средстве (автомобиле);\n",
            "определить, является ли выявленная неисправность, поломка или дефект в обследуемом транспортном средстве (авто) приобретенной в ходе неправильной эксплуатации автомашины, либо же данная неисправность (дефект, недостаток) имеет заводской характер;\n",
            "определить время (момент) возникновения неисправности, поломки или дефекта относительно ДТП (до или после);\n",
            "определить причину возникновения неисправности, поломки или дефекта относительно характера возникновения (заводской дефект транспортного средства или дефект в процессе эксплуатации автомобиля);\n",
            "определить причину возникновения неисправности, поломки или дефекта относительно причины возникновения (с чем именно произошло столкновение);\n",
            "установить в период какого технологического заводского цикла дефекты могли возникнуть (в момент проектирования или конструирования автомобиля, либо в момент литья узлов и деталей, либо в момент некачественной или неправильной сборки узлов и агрегатов автомашины) если неисправность, поломка или дефект, имеет ярко выраженное заводское происхождение\n",
            "установить в чем конкретно выражена неправильная эксплуатация транспортного средства, если неисправности, поломки или дефекты, являются приобретенными в ходе неправильной эксплуатации транспортного средства\n",
            "установить причинно-следственные связи между неисправностью и ДТП, а также обстоятельствами, способствующими возникновению неисправностей\n",
            "установить техническую возможность предотвращения ДТП (наезда, столкновения, потери устойчивости и т.п.) при определенном техническом состоянии ТС, их отдельных узлов, механизмов, систем, агрегатов, в момент ДТП\n",
            "установить обстоятельства, связанные с техническим состоянием ТС, которые способствовали или могли способствовать возникновению ДТП.\n",
            "\n",
            "\n",
            " ===========================================: \n",
            "3880 токенов использовано на вопрос.\n",
            "\n",
            " ===========================================: \n",
            "4098 токенов использовано всего (вопрос-ответ).\n",
            "\n",
            " ===========================================: \n",
            "ЦЕНА запроса с ответом : 0.008196  $\n",
            "\n",
            " ===========================================: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(answer2)\n",
        "print()\n",
        "print(answer3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6lG4xwDjWhz",
        "outputId": "8c248fa8-d977-4566-98da-416b893b741e"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ОТВЕТ : \n",
            " Да, мы проводим транспортно-трасологическую экспертизу. Эта услуга включает исследование следов различных видов\n",
            " транспорта и механизмы их образования, а также определение различных параметров и обстоятельств ДТП.\n",
            "\n",
            "ОТВЕТ : \n",
            " Да, наша компания проводит транспортно-трасологическую экспертизу. Эта экспертиза включает в себя исследование\n",
            " дорожно-транспортных ситуаций, параметров движения транспортных средств, а также исследование следов различных видов\n",
            " транспорта и механизмов их образования. Мы можем помочь вам определить механизм столкновения транспортных средств,\n",
            " последовательность возникновения повреждений, угол взаимного расположения ТС, траекторию и характер движения ТС, а\n",
            " также другие вопросы, связанные с\n"
          ]
        }
      ]
    }
  ]
}